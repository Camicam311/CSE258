{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "\n",
      "The most reviewed styles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Fruit / Vegetable Beer', 1355],\n",
       " ['American Pale Ale (APA)', 2288],\n",
       " ['Euro Pale Lager', 701],\n",
       " ['American Porter', 2230],\n",
       " ['Doppelbock', 873],\n",
       " ['American Barleywine', 825],\n",
       " ['English Pale Ale', 1324],\n",
       " ['Rauchbier', 1938],\n",
       " ['American IPA', 4113],\n",
       " ['American Double / Imperial IPA', 3886],\n",
       " ['Russian Imperial Stout', 2695],\n",
       " ['American Double / Imperial Stout', 5964],\n",
       " ['Scotch Ale / Wee Heavy', 2776],\n",
       " ['Old Ale', 1052],\n",
       " ['Czech Pilsener', 1501],\n",
       " ['Rye Beer', 1798]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def parseData(fname):\n",
    "  for l in open(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = list(parseData(\"beer.json\"))\n",
    "\n",
    "# Returns two dictionaries, with the average rating for each style, \n",
    "# and #reviews for each style\n",
    "def get_average_and_review_count(data):\n",
    "    counts = dict()\n",
    "    average = dict()\n",
    "\n",
    "    for data_point in data:\n",
    "        style = data_point['beer/style']\n",
    "        review = data_point['review/taste']\n",
    "        n = counts.get(style, 0)\n",
    "        old_average = average.get(style, 0)\n",
    "\n",
    "        counts[style] = n + 1\n",
    "        average[style] = (old_average * n + review) / (n+1)\n",
    "    return average, counts\n",
    "\n",
    "average, counts = get_average_and_review_count(data)\n",
    "print \"\\nThe most reviewed styles\"\n",
    "[[style, counts[style]] for style in counts.keys() if counts[style] >= 700]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average reviews for each style, where average is >= 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Belgian Strong Pale Ale', 4.05617088607595],\n",
       " ['American Porter', 4.081838565022416],\n",
       " ['Wheatwine', 4.186813186813188],\n",
       " ['American Barleywine', 4.064242424242424],\n",
       " ['Rauchbier', 4.067853457172355],\n",
       " ['Baltic Porter', 4.213035019455248],\n",
       " ['American IPA', 4.000850960369554],\n",
       " ['English Barleywine', 4.360902255639096],\n",
       " ['American Double / Imperial IPA', 4.033324755532658],\n",
       " ['American Wild Ale', 4.188775510204083],\n",
       " ['Russian Imperial Stout', 4.300371057513916],\n",
       " ['American Double / Imperial Stout', 4.479963112005356],\n",
       " ['Scotch Ale / Wee Heavy', 4.08339337175794],\n",
       " ['Old Ale', 4.096007604562735],\n",
       " ['Rye Beer', 4.213570634037825]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"\\nAverage reviews for each style, where average is >= 4\"\n",
    "[[style, average[style]] for style in average.keys() if average[style] >= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta:\n",
      "[ 3.55048115  0.20326687]\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def feature(d):\n",
    "    feat = [1]\n",
    "    if d['beer/style'] == 'American IPA':\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def f(theta, X, y, lam):\n",
    "    theta = np.matrix(theta).T\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "\n",
    "    diff = X*theta - y \n",
    "    diffSq = diff.T * diff\n",
    "    diffSqReg = diffSq / len(X) + lam*(theta.T*theta)\n",
    "    res = diffSqReg.flatten().tolist()\n",
    "    \n",
    "    return res\n",
    "\n",
    "# Derivate of f \n",
    "def fprime(theta, X, y, lam):\n",
    "    theta = np.matrix(theta).T\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "    \n",
    "    diff = X*theta - y\n",
    "    res = 2*X.T*diff / len(X) + 2*lam*theta\n",
    "    res = np.array(res.flatten().tolist()[0])\n",
    "    \n",
    "    return res\n",
    "# Extract wanted features for X\n",
    "X = [feature(d) for d in data]\n",
    "# We are calculating the review/taste value\n",
    "y = [d['review/taste'] for d in data]\n",
    "theta = [0,0]\n",
    "# Running gradient descent on the data\n",
    "res = optimize.fmin_l_bfgs_b(f, [0,0], fprime, args = (X, y, 0.1))\n",
    "print \"\\nTheta:\"\n",
    "print res[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_0 = 3.55048115, \\theta_1 = 0.20326687$\n",
    "\n",
    "$\\theta_0$ represents the average for a beer that is not an American IPA\n",
    "\n",
    "$\\theta_1$ represents the extra rating a American IPA usually has compared to the average rating.\n",
    "\n",
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta\n",
      "[ 3.53845571  0.19558705]\n",
      "\n",
      "MSE Training data\n",
      "[[ 0.68485066]]\n",
      "\n",
      "MSE Testing data\n",
      "[[ 0.61342104]]\n"
     ]
    }
   ],
   "source": [
    "# MSE error \n",
    "def square_error(theta,X,y):\n",
    "    theta = np.matrix(theta).T\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "    \n",
    "    diff = X*theta - y \n",
    "    diffSq = diff.T * diff\n",
    "    \n",
    "    return diffSq / len(X)\n",
    "\n",
    "training_data = data[0:n]\n",
    "testing_data = data[n::]\n",
    "\n",
    "# Extracting wanted features for X\n",
    "X_train = [feature(d) for d in training_data]\n",
    "X_test = [feature(d) for d in testing_data]\n",
    "\n",
    "# Extracting feature we are calculating\n",
    "y_train = [d['review/taste'] for d in training_data]\n",
    "y_test = [d['review/taste'] for d in testing_data]\n",
    "\n",
    "theta = [0, 0]\n",
    "\n",
    "# Gradient descent on training data\n",
    "theta = optimize.fmin_l_bfgs_b(f, theta, fprime, args = (X_train, y_train, 0.1))[0]\n",
    "print\"Theta\"\n",
    "print theta\n",
    "\n",
    "print \"\\nMSE Training data\"\n",
    "print square_error(theta, X_train, y_train)\n",
    "\n",
    "print \"\\nMSE Testing data\"\n",
    "print square_error(theta, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta values\n",
      "[  3.31586548e+00   4.97476833e-03  -2.38612413e-02   2.51161620e-02\n",
      "   1.65662022e-01   1.25274089e-02  -1.22513767e-02   4.49887739e-02\n",
      "   2.63189656e-01   6.66707762e-01   2.98763600e-01   9.72161032e-02\n",
      "   2.77190596e-03   1.43582944e-01   8.41279990e-02   5.49112449e-03\n",
      "   3.29193380e-02   1.32546873e-03   2.77272013e-02   2.29714851e-02\n",
      "   2.13864740e-02   1.50516596e-01   2.91581529e-02  -1.35037453e-02\n",
      "  -1.21042949e-04  -5.40085121e-03   1.55368384e-02   3.66970067e-02\n",
      "   1.64031407e-02   1.90976485e-02   9.90036725e-03  -1.29586827e-03\n",
      "   4.44001761e-02   7.88522603e-03   2.38102761e-02   8.25645121e-03\n",
      "  -6.41941592e-02  -4.16144065e-02   2.75815410e-02   1.07770525e-01\n",
      "   3.07321525e-02  -1.07558062e-02   1.24299751e-02  -1.51895491e-01\n",
      "   2.39937542e-03   9.03973362e-03   6.76760992e-02   1.43895635e-02\n",
      "   5.10720829e-01   3.03164693e-02   1.73990850e-02   4.02272347e-01\n",
      "   1.37394281e-02  -3.90780174e-03   2.43513397e-02   2.38629070e-02\n",
      "   1.13014266e-02]\n",
      "\n",
      "MSE Error Training\n",
      "[[ 0.54992619]]\n",
      "\n",
      "MSE Error Testing\n",
      "[[ 0.65875293]]\n"
     ]
    }
   ],
   "source": [
    "# We get the amount of reviews for each style based on our training data\n",
    "review_count = get_average_and_review_count(training_data)[1]\n",
    "# Filter out each style where #reviews >= 50\n",
    "styles = sorted(list(set([d['beer/style'] for d in data if review_count[d['beer/style']] >= 50])))\n",
    "\n",
    "theta = [0 for x in range(len(styles)+1)]\n",
    "\n",
    "# Extracting features for X\n",
    "def feature_all(d, styles):\n",
    "    feat = [1]\n",
    "    for style in styles:\n",
    "        if d['beer/style'] == style:\n",
    "            feat.append(1)\n",
    "        else:\n",
    "            feat.append(0)\n",
    "    return feat\n",
    "\n",
    "# Training and test data for X\n",
    "X_train = [feature_all(d, styles) for d in training_data]\n",
    "X_test = [feature_all(d,styles) for d in testing_data]\n",
    "\n",
    "# Gradient descent\n",
    "theta = optimize.fmin_l_bfgs_b(f, theta, fprime, args=(X_train, y_train, 0.1))[0]\n",
    "\n",
    "print \"Theta values\"\n",
    "print theta\n",
    "\n",
    "print \"\\nMSE Error Training\"\n",
    "print square_error(theta, X_train, y_train)\n",
    "\n",
    "print \"\\nMSE Error Testing\"\n",
    "print square_error(theta, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy training\n",
      "0.9226\n",
      "\n",
      "Accuray Test-set\n",
      "0.85632\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def SVM_execute(X, y, C=1000):\n",
    "    n = len(X)\n",
    "    # Setting up training & Test data\n",
    "    X_train = X[:n//2]\n",
    "    X_test = X[n//2::]\n",
    "    y_train = y[:n//2]\n",
    "    y_test = y[n//2::]\n",
    "    \n",
    "    # Setting up the SVM\n",
    "    clf = svm.SVC(C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Retrieving predictions from trained SVM\n",
    "    train_predictions = clf.predict(X_train)\n",
    "    test_predictions = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    percentage_train = sum([ y_train[i] == train_predictions[i] for i in range(n//2)]) / float(n//2)\n",
    "    percentage_test =  sum([ y_test[i] == test_predictions[i] for i in range(n//2)]) / float(n//2)\n",
    "    \n",
    "    print \"\\nAccuracy training\"\n",
    "    print percentage_train\n",
    "    print \"\\nAccuray Test-set\"\n",
    "    print percentage_test\n",
    "\n",
    "# Extracting wanted features\n",
    "X = [[d['beer/ABV'], d['review/taste']] for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]\n",
    "\n",
    "SVM_execute(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy training\n",
      "0.94412\n",
      "\n",
      "Accuray Test-set\n",
      "0.95536\n"
     ]
    }
   ],
   "source": [
    "def feature_extractor(d):\n",
    "    return [\n",
    "        d['beer/ABV'],\n",
    "        'IPA' in d['review/text']\n",
    "        \n",
    "    ]\n",
    "\n",
    "X = [feature_extractor(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]\n",
    "\n",
    "SVM_execute(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a feature vector cointaining the ABV and wether or not the review text contains 'IPA' in the review text gives the result\n",
    "\n",
    "\n",
    "Accuracy training:\n",
    "0.94412\n",
    "\n",
    "Accuray Test-set:\n",
    "0.95536\n",
    "### Task 7\n",
    "\n",
    "\n",
    "The regularization constant penalizes the complexity model. With a high regularization constant penalizes high complexity models, forcing the SVM to choose a simpler model.\n",
    "\n",
    "The accuracy from the different regularization constants:\n",
    "\n",
    "Regularization Constant | Training Data | Test data\n",
    "---|---|---|---|---\n",
    "C=0.1 | 0.94508 | 0.95704\n",
    "C=10 |0.94416 | 0.95752\n",
    "C=1000 | 0.94412 | 0.95536\n",
    "C=100000 | 0.9442 | 0.9556\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Regularization constant = 0.1\n",
      "\n",
      "Accuracy training\n",
      "0.94508\n",
      "\n",
      "Accuray Test-set\n",
      "0.95704\n",
      "--\n",
      "Regularization constant = 10\n",
      "\n",
      "Accuracy training\n",
      "0.94416\n",
      "\n",
      "Accuray Test-set\n",
      "0.95752\n",
      "--\n",
      "Regularization constant = 1000\n",
      "\n",
      "Accuracy training\n",
      "0.94412\n",
      "\n",
      "Accuray Test-set\n",
      "0.95536\n",
      "--\n",
      "Regularization constant = 100000\n",
      "\n",
      "Accuracy training\n",
      "0.9442\n",
      "\n",
      "Accuray Test-set\n",
      "0.9556\n"
     ]
    }
   ],
   "source": [
    "X = [feature_extractor(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]\n",
    "\n",
    "print \"--\\nRegularization constant = 0.1\"\n",
    "SVM_execute(X,y,0.1)\n",
    "\n",
    "print \"--\\nRegularization constant = 10\"\n",
    "SVM_execute(X,y,10)\n",
    "\n",
    "print \"--\\nRegularization constant = 1000\"\n",
    "SVM_execute(X,y,1000)\n",
    "\n",
    "print \"--\\nRegularization constant = 100000\"\n",
    "SVM_execute(X,y,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $f$ can be written as\n",
    "\n",
    "$\\sum_{i=1}^{n} -log(1 + e^{-x_i\\theta}) - X \\theta - \\lambda||\\theta||_2^2$\n",
    "\n",
    "Where the second term is only considered if $y[i] = False$\n",
    "\n",
    "The final log-likelihood is $14627.2048715$\n",
    "\n",
    "The accuracy is $0.9218$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final log likelihood = 14627.2048715\n",
      "Accuracy =  0.9218\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from math import log\n",
    "import random\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0.0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = - inner(X[i], theta)\n",
    "    res = np.inner(X[i], exp(logit)) / ( 1 + exp(logit))\n",
    "    if not y[i]:\n",
    "        res -= X[i]\n",
    "    dl += res\n",
    "  for i in range(len(theta)):\n",
    "    dl[i] -= 2*lam*theta[i]\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "  return np.array([-x for x in dl])\n",
    "\n",
    "\n",
    "X = [[d['beer/ABV'], d['review/taste']] for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]\n",
    "\n",
    "X_train = X[:len(X)/2]\n",
    "X_test = X[len(X)/2:]\n",
    "y_train = y[:len(X)/2]\n",
    "y_test = y[len(X)/2:]\n",
    "\n",
    "# Run gradient descent\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, y_train, 1.0))\n",
    "\n",
    "predictions = [sigmoid(inner(X_test[i], theta)) > 0.5 for i in range(len(X_test))]\n",
    "\n",
    "result = [ predictions[i] == y_test[i] for i in range(len(X_test))]\n",
    "acc = sum(result) / float(len(result))\n",
    "\n",
    "print \"Final log likelihood =\", f(theta,X,y,1.0)\n",
    "print \"Accuracy = \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
